from datetime import datetime
from langchain_community.llms import Ollama
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import initialize_agent, Tool, AgentType
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os   

load_dotenv()  # Carrega vari√°veis de ambiente do arquivo .env

# =========  MODELO LOCAL (Llama3 via Ollama) =========
system_prompt = """
Voc√™ √© um agente inteligente que segue o formato ReAct (Racioc√≠nio + A√ß√£o).

Use o formato abaixo para cada intera√ß√£o:

Thought: descreva brevemente o que voc√™ vai fazer.
Action: escolha UMA ferramenta (entre as dispon√≠veis) para obter informa√ß√µes ou realizar c√°lculos.
Action Input: passe o input necess√°rio para a ferramenta.

Ap√≥s receber a resposta da ferramenta, pense novamente (Thought) e decida:
- Se ainda precisar buscar algo ‚Üí use outra Action.
- Se j√° tiver todas as informa√ß√µes ‚Üí finalize com:

Final Answer: [resposta completa ao usu√°rio em portugu√™s]

‚ö†Ô∏è Regras importantes:
- Nunca escreva "Action: None".
- Sempre que a resposta estiver pronta, finalize com "Final Answer:".
- Evite repetir "Thought:Thought:". Use apenas um "Thought:" por vez.
- Mantenha o racioc√≠nio breve e direto.
- Se o usu√°rio fizer mais de uma pergunta na mesma mensagem,
responda **todas elas**, usando a mem√≥ria da conversa quando necess√°rio.
"""



llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.1,
    api_key=os.getenv("API_KEY")
)




    # =========  FERRAMENTAS =========
search = DuckDuckGoSearchRun()

from langchain.prompts import ChatPromptTemplate

def traduz(expressao: str) -> str:
    try:
        # Cria um prompt personalizado apenas para este uso
        prompt = ChatPromptTemplate.from_template("""
        Voc√™ √© um tradutor de idiomas.
        Traduza o texto para portugues brasileiro
        Texto: {texto}
        """)

        # Monta a cadeia (prompt ‚Üí llm)
        chain = prompt | llm  # isso combina o template com o modelo

        # Executa apenas esse fluxo
        resposta = chain.invoke({"texto": expressao})
        return resposta.content

    except Exception as e:
        return f"Erro ao traduzir: {e}"

def calcular(expressao: str) -> str:
    try:
        resultado = eval(expressao)
        return f"O resultado de {expressao} √© {resultado}"
    except Exception as e:
        return f"Erro ao calcular: {e}"

def dataHoje(*args, **kwargs) -> str:
    try:
        data_hoje = datetime.now().strftime("%d/%m/%Y")
        return data_hoje
    except Exception as e:
        return f"Erro ao pegar data: {e}"
    
tools = [
    Tool(
        name="BuscaWeb",
        func=search.run,
        description="Use para buscar informa√ß√µes atuais na internet."
    ),
    Tool(
        name="Calculadora",
        func=calcular,
        description="Use para c√°lculos matem√°ticos simples."
    ),
    Tool(
        name="Tradutor",
        func=traduz,
        description="Use para traduzir textos para portugu√™s brasileiro, sempre que o input n√£o estiver em portugu√™s."
    ),
    Tool(
        name="DataHoje",
        func=dataHoje,
        description="sempre que o usu√°rio perguntar algo que envolva a data atual, utilize essa ferramenta para responder com a data de hoje."
    )
]

# =========  MEM√ìRIA =========
# Armazena o hist√≥rico das conversas (mensagens anteriores)
memory = ConversationBufferMemory(
    memory_key="chat_history",  # nome da vari√°vel usada pelo agente
    return_messages=True
)

# =========  AGENTE COM MEM√ìRIA =========
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,  # ReAct com mem√≥ria
    memory=memory,
    verbose=True,
    handle_parsing_errors=True,
)

# =========  INTERA√á√ÉO =========
print("ü§ñ Agente iniciado! Digite 'sair' para encerrar.\n")

while True:
    pergunta = input("Voc√™: ")
    if pergunta.lower() in ["sair", "exit", "quit"]:
        break

    resposta = agent.run(pergunta)
    print("Agente:", resposta)
